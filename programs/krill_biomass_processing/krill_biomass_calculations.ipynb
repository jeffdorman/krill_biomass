{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyreadr - used to read R data into python\n",
    "#pip install oceansdb - worked to install ocean depth data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as st\n",
    "import time as time\n",
    "import pyreadr\n",
    "import oceansdb\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "#Load NASC Data\n",
    "#fname = '../../data/acoustic_reprocessing/JRS_2014_reprocessed/JRS_2014_reprocessed_output/20140530_reprocessed_depth_integrated.csv'\n",
    "fname='../../data/acoustic_reprocessing/JRS_2014_reprocessed/JRS_2014_reprocessed_output/20140530_reprocessed_depth_integrated_10rows.csv'\n",
    "df1=pd.read_csv(fname)\n",
    "df1=df1.replace([-9999.0, 9999.0, -999.0, 999.0], np.nan)\n",
    "\n",
    "\n",
    "#Load Length Frequency Data\n",
    "#Euphausia pacifica\n",
    "fname = '../../data/LF_data/epClimatology_edit.csv'\n",
    "epLF=pd.read_csv(fname)\n",
    "epLF=epLF.replace(np.nan, 0)\n",
    "#Thysanoessa spinifera\n",
    "fname = '../../data/LF_data/tsClimatology_edit.csv'\n",
    "tsLF=pd.read_csv(fname)\n",
    "tsLF=tsLF.replace(np.nan, 0)\n",
    "#Nematoscelis Difficilis\n",
    "fname = '../../data/LF_data/ndClimatology_edit.csv'\n",
    "ndLF=pd.read_csv(fname)\n",
    "ndLF=tsLF.replace(np.nan, 0)\n",
    "\n",
    "#Load TS/BS Data\n",
    "#Euphausia pacifica\n",
    "fname='../../../R/data/20200528_results/mean_ep_BS_data_20200528.csv'\n",
    "#result=pyreadr.read_r(fname)\n",
    "epTSBS=pd.read_csv(fname, header=None)\n",
    "#dfTS.rename(columns={\"0\":\"length\",\"1\":\"mean\",\"2\":\"std_dev\", \"3\":\"std_err\"})\n",
    "#Thysanoessa spinifera\n",
    "fname='../../../R/data/20200528_results/mean_ts_BS_data_20200528.csv'\n",
    "tsTSBS=pd.read_csv(fname, header=None)\n",
    "#Nematoscelis difficilis\n",
    "fname='../../../R/data/20200528_results/mean_nd_BS_data_20200528.csv'\n",
    "ndTSBS=pd.read_csv(fname, header=None)\n",
    "\n",
    "#Load Species Composition Data\n",
    "fname='../../data/species_composition/Predictive_Depth_GAM.csv'\n",
    "SC=pd.read_csv(fname)\n",
    "\n",
    "\n",
    "#Calculates DW for the size classes of each species (10 to 35 mm)\n",
    "epDW=(epTSBS.loc[0:20,0]**3.239)*.000795\n",
    "#tsDW=\n",
    "#ndDW=\n",
    "\n",
    "\n",
    "\n",
    "#Set Up Regions\n",
    "#Region 1 - Southern California Onshore\n",
    "#fname=\"../../data/regions_200m_isobath/CA_region_southern_nearshore.shp\"\n",
    "#r1_shapefile = gpd.read_file(fname)\n",
    "#r1 = Polygon(r1_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 2 - Southern California Offshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_southern_offshore.shp\"\n",
    "r2_shapefile = gpd.read_file(fname)\n",
    "r2 = Polygon(r2_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 3 - Southern California Onshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_central_nearshore.shp\"\n",
    "r3_shapefile = gpd.read_file(fname)\n",
    "r3 = Polygon(r3_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 4 - Southern California Offshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_central_offshore.shp\"\n",
    "r4_shapefile = gpd.read_file(fname)\n",
    "r4 = Polygon(r4_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 5 - Southern California Onshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_northcentral_nearshore.shp\"\n",
    "r5_shapefile = gpd.read_file(fname)\n",
    "r5 = Polygon(r5_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 6 - Southern California Offshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_northcentral_offshore.shp\"\n",
    "r6_shapefile = gpd.read_file(fname)\n",
    "r6 = Polygon(r6_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 7 - Southern California Onshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_northern_nearshore.shp\"\n",
    "r7_shapefile = gpd.read_file(fname)\n",
    "r7 = Polygon(r7_shapefile.iloc[0,5])\n",
    "\n",
    "#Region 8 - Southern California Offshore\n",
    "fname=\"../../data/regions_200m_isobath/CA_region_northern_offshore.shp\"\n",
    "r8_shapefile = gpd.read_file(fname)\n",
    "r8 = Polygon(r8_shapefile.iloc[0,5])\n",
    "\n",
    "#r1_lon=np.array([-115, -130, -130, -115])\n",
    "#r1_lat=np.array([20, 20, 34.5, 34.5])\n",
    "#r1_lon_lat_vect = np.column_stack((r1_lon, r1_lat)) # Reshape coordinates\n",
    "#r1 = Polygon(r1_lon_lat_vect) # create polygon\n",
    "#Region 2 - Central California \n",
    "#r2_lon=np.array([-115, -130, -130, -115])\n",
    "#r2_lat=np.array([34.5, 34.5, 36.3, 36.3 ])\n",
    "#r2_lon_lat_vect = np.column_stack((r2_lon, r2_lat)) # Reshape coordinates\n",
    "#r2 = Polygon(r2_lon_lat_vect) # create polygon\n",
    "#Region 3 - North Central California \n",
    "#r3_lon=np.array([-115, -130, -130, -115])\n",
    "#r3_lat=np.array([36.3, 36.3, 38, 38 ])\n",
    "#r3_lon_lat_vect = np.column_stack((r3_lon, r3_lat)) # Reshape coordinates\n",
    "#r3 = Polygon(r3_lon_lat_vect) # create polygon\n",
    "#Region 4 - Northern California \n",
    "#r4_lon=np.array([-115, -130, -130, -115])\n",
    "#r4_lat=np.array([38, 38, 50, 50 ])\n",
    "#r4_lon_lat_vect = np.column_stack((r4_lon, r4_lat)) # Reshape coordinates\n",
    "#r4 = Polygon(r4_lon_lat_vect) # create polygon\n",
    "\n",
    "odb = oceansdb.ETOPO()\n",
    "\n",
    "\n",
    "#Calculations\n",
    "#epwmTS=sum(LF*TS)            #weighted mean TS\n",
    "#epbs=4*np.pi*10**(epwmTS/10) #individual krill backscattering strength\n",
    "#epnk=nasc/epbs               #number of krill\n",
    "#epnkLF=epnk*epLF             #number of krill by each length\n",
    "#epbiomass=sum(jeff*epDW)     #and krill Biomass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epTSBS\n",
    "epDW\n",
    "print(epLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n",
      "Already in Region\n"
     ]
    }
   ],
   "source": [
    "#Set Inital Region to where we have the most data (Northern Central California)\n",
    "reg=3  # This will get updated based on the lat lon data below\n",
    "\n",
    "#### 5/29/2020 - This is a KLUDGE to get the code working for testing...Needs to be removed!!!\n",
    "r1=r2 #####\n",
    "####\n",
    "\n",
    "for i in df1.index:\n",
    "    #Get NASC \n",
    "    nasc=df1[' NASC'][i]\n",
    "    \n",
    "    if np.isnan(nasc):  #If no data, move on\n",
    "        #update Counters and put nan's in the output file\n",
    "        continue\n",
    "    else:\n",
    "        #Get Longitude and Latitude\n",
    "        lon=df1[' Lon_M'][i]\n",
    "        lat=df1[' Lat_M'][i]\n",
    "    \n",
    "        if eval('r' + str(reg) +'.contains(Point(lon,lat))'):\n",
    "            #Already in Region, dont need to update\n",
    "            print('Already in Region')\n",
    "        else:\n",
    "            #Need to Loop Through all Regions to Determine the Region\n",
    "            if r1.contains(Point(lon,lat)):\n",
    "                reg=1\n",
    "            elif r2.contains(Point(lon,lat)):\n",
    "                reg=2\n",
    "            elif r3.contains(Point(lon,lat)):\n",
    "                reg=3\n",
    "            elif r4.contains(Point(lon,lat)):\n",
    "                reg=4\n",
    "            elif r5.contains(Point(lon,lat)):\n",
    "                reg=5\n",
    "            elif r6.contains(Point(lon,lat)):\n",
    "                reg=6\n",
    "            elif r7.contains(Point(lon,lat)):\n",
    "                reg=7\n",
    "            elif r8.contains(Point(lon,lat)):\n",
    "                reg=8\n",
    "            else:  # If no Region, move on\n",
    "                #No Valid Region was found\n",
    "                #update Counters and put nan's in the output file\n",
    "                continue\n",
    "    \n",
    "        #Determine the Depth\n",
    "        h = odb['topography'].extract(lat=lat, lon=lon)  #input needs to be lat=38, lon=-125\n",
    "        depth=np.int(round(h['height'][0])*-1)  \n",
    "    \n",
    "    \n",
    "        #Determine the Euphausia pacifica vs Thysanoessa spinifera split\n",
    "        #SC\n",
    "        if depth>3500:\n",
    "            cc_ratio=0\n",
    "        else:\n",
    "            cc_ratio=SC['Tspin_to_Epac_Ratio'][depth]\n",
    "            \n",
    "        #Determine the amount of NASC for each Group\n",
    "        ep_nasc=nasc*(1-cc_ratio)\n",
    "        ts_nasc=nasc*(cc_ratio)\n",
    "\n",
    "        #if past_region==region:\n",
    "        ep_reg_LF=epLF.loc[reg,\"10\":\"30\"].values\n",
    "        ts_reg_LF=tsLF.loc[reg,\"10\":\"30\"].values\n",
    "        \n",
    "        epBS=epTSBS.loc[0:20,1]\n",
    "        tsBS=tsTSBS.loc[0:20,1]\n",
    "    \n",
    "        #Calculations\n",
    "        epwmBS=sum(ep_reg_LF*epBS)            #weighted mean TS\n",
    "        #epBS=4*np.pi*10**(epwmTS/10) #individual krill backscattering strength\n",
    "        epnk=nasc/epwmBS               #number of krill\n",
    "        epnkLF=epnk*ep_reg_LF             #number of krill by each length\n",
    "        epbiomass=sum(epnkLF*epDW)     #and krill Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What percentage of the krill belong to Tspin and what percentage belong to Epac?\n",
    "#lat=df1[' Lat_M']\n",
    "#lon=df1[' Lon_M']\n",
    "lat=35\n",
    "lon=-125\n",
    "odb = oceansdb.ETOPO()\n",
    "h = odb['topography'].extract(lat, lon)\n",
    "\n",
    "#Formula for partitioning Tspin vs. Epac\n",
    "\n",
    "epnasc=500\n",
    "tsnasc=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What year and Region? \n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import geopandas as gpd\n",
    "shapefile = gpd.read_file(\"../../data/caorwall/caorwall.shp\")\n",
    "#print(shapefile)\n",
    "print(shapefile.head(2))\n",
    "%matplotlib inline\n",
    "shapefile.plot()\n",
    "#print(shapefile['INCREM'].head())\n",
    "loar=shapefile['ICREM']==200\n",
    "\n",
    "\n",
    "x=37\n",
    "y=-122\n",
    "lons_vect=np.array([-120, -124, -124, -120])\n",
    "lats_vect=np.array([35, 35, 40, 40])\n",
    "lons_lats_vect = np.column_stack((lons_vect, lats_vect)) # Reshape coordinates\n",
    "polygon = Polygon(lons_lats_vect) # create polygon\n",
    "point = Point(y,x) # create point\n",
    "print(polygon.contains(point)) # check if polygon contains point\n",
    "print(point.within(polygon)) # check if a point is in the polygon \n",
    "\n",
    "#Load up the TS table for Epac and Tspin\n",
    "#Load up the proper L/F Table (Dependent on year and region)\n",
    "\n",
    "#Calculate the mean TS:\n",
    "\n",
    "#year=df1[' Date_M']  #Only Need to do this once\n",
    "#region=\n",
    "#mts=-78.5\n",
    "lons_lats_vect\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

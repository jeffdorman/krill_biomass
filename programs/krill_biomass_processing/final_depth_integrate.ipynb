{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_depth_integrate(finname,foutname):\n",
    "    \n",
    "    #jupyter nbconvert --to script final_depth_integrate.ipynb \n",
    "    # Use the above script in a Terminal Window to convert to a .py file\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statistics as st\n",
    "    import time as time\n",
    "\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.ast_node_interactivity = \"last\"\n",
    "    #other options include 'none', 'last', 'last_expr'\n",
    "\n",
    "    df1=pd.read_csv(finname)  \n",
    "\n",
    "    df2=df1.copy(deep=True)\n",
    "    df2.drop(df2[df2[' Layer']!=2].index, inplace=True)\n",
    "    df2=df2.reset_index(drop=True)\n",
    "    \n",
    "    df1=df1.replace([-9999.0, 9999.0, -999.0, 999.0], np.nan)\n",
    "    df2=df2.replace([-9999.0, 9999.0, -999.0, 999.0], np.nan)\n",
    "    #df1=df1.replace(9999.0, np.nan)\n",
    "    #df2=df2.replace(-9999.0, np.nan)\n",
    "    #df2=df2.replace(9999.0, np.nan)\n",
    "\n",
    "    df1_interval=np.nanmax(df1[' Interval'])\n",
    "    df2_interval=np.nanmax(df2[' Interval'])\n",
    "    mx_interval_int=int(df1_interval)\n",
    "\n",
    "    #Check that the number or instances with data (# of intervals) equals the number of rows in df2\n",
    "    if len(np.unique(df1[' Interval'])) != len(df2.index):\n",
    "        print('Mismatch in Length of Files!!! ' +finname+ ' NOT processed')  \n",
    "    else:      \n",
    "        bad_value=-9998\n",
    "        tic=time.time()\n",
    "        cnt=0  #Counter is needed in case interval is not sequential in the original csv file\n",
    "        for i in range (mx_interval_int+1):\n",
    "            if any(df1[' Interval']==i):\n",
    "                #print(i)\n",
    "                loar=df1[' Interval']==i\n",
    "                #idx=loar[loar==True].index[-1]  #Maybe Not Needed\n",
    "                #df2[' NASC'][i]=sum((df1[' NASC'])[loar])  #THis Created Warnings!  Better to use iloc like below\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Sv_mean')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' NASC')]=sum((df1[' NASC'])[loar])\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Sv_max')]=np.nanmax((df1[' Sv_max'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Sv_min')]=np.nanmin((df1[' Sv_min'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Sv_noise')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' NASC_noise')]=bad_value\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Height_mean')]=sum((df1[' Height_mean'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Depth_mean')]=bad_value\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Samples')]=sum((df1[' Samples'])[loar])\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Layer_depth_max')]=np.nanmax((df1[' Layer_depth_max'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Layer_depth_min')]=np.nanmin((df1[' Layer_depth_min'])[loar])\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Standard_deviation')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Skewness')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Kurtosis')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' ABC')]=sum((df1[' ABC'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Area_Backscatter_Strength')]=bad_value\n",
    "\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Thickness_mean')]=sum((df1[' Thickness_mean'])[loar])\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Range_mean')]=bad_value\n",
    "                df2.iloc[cnt,df2.columns.get_loc(' Beam_volume_sum')]=sum((df1[' Beam_volume_sum'])[loar])\n",
    "                cnt=cnt+1\n",
    "            #tmp_date=df[' Date_M'][loar]\n",
    "            #f_time.append((df[' Time_M'])[loar])\n",
    "    toc=time.time()\n",
    "    elapsed=toc-tic\n",
    "    #print(elapsed)\n",
    "    \n",
    "    df2=df2.fillna(value=-9999.0)\n",
    "    df2\n",
    "    df2.to_csv (foutname, index = False, header=True)\n",
    "    print('Writing ' +foutname+ ' with ' +str(len(df2.index))+ ' rows.') \n",
    "    print('Processing took ' +str(elapsed)+ ' seconds.')\n",
    "    print('')\n",
    "          \n",
    "    #import csv\n",
    "    #csvData=[f_lon, f_lat, f_nasc]\n",
    "\n",
    "    #zipped=zip(f_date,f_time,f_lon, f_lat, f_nasc)\n",
    "    #zipped=zip(f_time,f_lon, f_lat, f_nasc)\n",
    "\n",
    "    #with open('test.csv', 'w') as csvFile:\n",
    "    #    writer=csv.DictWriter(csvFile, fieldnames=[\"Time\",\"Lon_M\",\"Lat_M\",\"NASC\"])\n",
    "    #    writer.writeheader()\n",
    "    #    writer = csv.writer(csvFile)\n",
    "        #writer.writeheader\n",
    "    #    writer.writerows(zipped)\n",
    "\n",
    "    #csvFile.close()\n",
    "\n",
    "    #type(f_lon)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
